{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace('sam')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Now install a schema from the OEA schema catalog. \r\n",
        "# In this example we're installing the Microsoft Education Insights schema. Note that you can choose to install the schema with or without test data.\r\n",
        "# For more details on how this method works, see the source code in the OEA_py notebook.\r\n",
        "oea.install_schema('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/schemas/schema_catalog/Microsoft_Education_Insights/Microsoft_Education_Insights.sss.csv', 'stage2/Refined/contoso2/v0.1', True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) You can see the new lake db listed in the UI (click on Data in the left nav, then Workspace), and work with tables directly from that UI element, \r\n",
        "# or you can work with the tables directly in this notebook.\r\n",
        "df = spark.sql(\"select * from ldb_sam_s2r_contoso2_v0p1.person\")\r\n",
        "display(df)\r\n",
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that you have the schema installed, you can test out any modules that specify this schema as an input depency.\r\n",
        "# This is especially helpful when you want to utilize an existing dashboard because you can install the schema it is dependent on and then build out the pipelines to hydrate that schema."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}