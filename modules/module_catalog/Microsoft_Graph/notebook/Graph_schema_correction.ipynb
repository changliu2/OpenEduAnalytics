{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Graph Module Ingestion - Schema Correction\n",
        "\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, while correcting module tables initially ingested as un-flattened, with incorrect column data types, and lacking unique primary keys.\n",
        "\n",
        "Tables are read from ```stage2/Ingested/graph_api/(beta or v1.0)``` and written out, with the corrected schema, to ```stage2/Ingested_Corrected/graph_api/(beta or v1.0)```\n",
        "\n",
        "The steps outlined below describe how this notebook is used to correct the Microsoft Graph Reports API module tables:\n",
        "- Set the workspace for where the table schemas are to be corrected. \n",
        "- 5 functions are defined and used:\n",
        "   1. **_correct_users_table**: flattens the users table.\n",
        "   2. **_correct_m365_table**: flattens, corrects some column dtypes, and adds a primary key per row for the m365_app_user_detail table.\n",
        "   3. **_correct_teams_table**: flattens, corrects some column dtypes, and adds a primary key per row some column dtypes for the teams_activity_user_detail table.\n",
        "   4. **_correct_meetings_table**: flattens, corrects some column dtypes, and adds a primary key per row for the meeting_attendance_report table.\n",
        "   5. **correct_graph_dataset**: extracts the names of all the folders currently stored in stage2/Ingested/graph_api, corrects the schema per table using the functions above, and overwrites the tables with the updated schemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "workspace = 'dev'\n",
        "testdataSet = 'hed'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run OEA_py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\n",
        "oea.set_workspace(workspace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# 2) schema correction, since Graph data initially landed is unstructured with nested arrays, incorrect column dtypes, and without primary keys for 3 of the 4 tables.\n",
        "def _correct_users_table(df):\n",
        "    df_flat = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\n",
        "    return df_flat\n",
        "\n",
        "def _correct_m365_table(df):\n",
        "    df_flat = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\n",
        "    df_flat = df_flat.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\n",
        "                    .withColumn('mobile', F.explode(F.col('details').mobile)) \\\n",
        "                    .withColumn('web', F.explode(F.col('details').web)) \\\n",
        "                    .withColumn('mac', F.explode(F.col('details').mac)) \\\n",
        "                    .withColumn('windows', F.explode(F.col('details').windows)) \\\n",
        "                    .withColumn('excel', F.explode(F.col('details').excel)) \\\n",
        "                    .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\n",
        "                    .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\n",
        "                    .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\n",
        "                    .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\n",
        "                    .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\n",
        "                    .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\n",
        "                    .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\n",
        "                    .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\n",
        "                    .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\n",
        "                    .withColumn('outlook', F.explode(F.col('details').outlook)) \\\n",
        "                    .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\n",
        "                    .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\n",
        "                    .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\n",
        "                    .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\n",
        "                    .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\n",
        "                    .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\n",
        "                    .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\n",
        "                    .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\n",
        "                    .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\n",
        "                    .withColumn('teams', F.explode(F.col('details').teams)) \\\n",
        "                    .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\n",
        "                    .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\n",
        "                    .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\n",
        "                    .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\n",
        "                    .withColumn('word', F.explode(F.col('details').word)) \\\n",
        "                    .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\n",
        "                    .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\n",
        "                    .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\n",
        "                    .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\n",
        "                    .drop('details')\n",
        "    # temp: add unique primary key per row, by combining UPNs with the date the report was generated\n",
        "    # this assumes every person has only one row per reportRefreshDate\n",
        "    df_flat = df_flat.withColumn('m365Activity_pk', F.concat(F.col('userPrincipalName'),F.lit('_'),F.col('reportRefreshDate')))\n",
        "\n",
        "    df_flat.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\n",
        "    df_flat.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\n",
        "    df_flat.select(F.col('lastActivationDate'), F.to_date(F.col('lastActivationDate'), 'yyyy-MM-dd'))\n",
        "    return df_flat\n",
        "\n",
        "def _correct_teams_table(df):\n",
        "    df_flat = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\n",
        "    df_flat = df_flat.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\n",
        "    # convert duration to seconds only \n",
        "    # NOTE: The duration expression may have changed and this will need to be modified to accommodate any new duration formatting\n",
        "    df_flat = df_flat.withColumn(\n",
        "        'screenShareDuration', \n",
        "        F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \n",
        "        F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \n",
        "        F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\n",
        "        ).withColumn(\n",
        "        'videoDuration', \n",
        "        F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \n",
        "        F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \n",
        "        F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\n",
        "        ).withColumn(\n",
        "        'audioDuration', \n",
        "        F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \n",
        "        F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \n",
        "        F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\n",
        "        )\n",
        "    # temp: add unique primary key per row, by combining UPNs with the date the report was generated\n",
        "    # this assumes every person has only one row per reportRefreshDate\n",
        "    df_flat = df_flat.withColumn('teamsActivity_pk', F.concat(F.col('userPrincipalName'),F.lit('_'),F.col('reportRefreshDate')))\n",
        "\n",
        "    df_flat.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\n",
        "    df_flat.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\n",
        "    return df_flat\n",
        "\n",
        "def _correct_meetings_table(df):\n",
        "    df_flat = df.select(\n",
        "        \"id\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\",\n",
        "        F.explode(\"attendanceRecords\").alias(\"attendanceRecordsExplode\")\n",
        "        ).select(\"id\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \n",
        "                \"attendanceRecordsExplode.*\")\n",
        "    df_flat = df_flat.withColumnRenamed(\"id\",\"meetingId\")\n",
        "    df_flat = df_flat.select(\n",
        "        \"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \n",
        "        \"totalAttendanceInSeconds\", \"role\", \"emailAddress\", \"attendanceIntervals\",\n",
        "        F.explode(F.array(\"identity\")).alias(\"identityExplode\")\n",
        "        ).select(\"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\"attendanceIntervals\",\n",
        "            \"identityExplode.*\")\n",
        "\n",
        "    df_flat = df_flat.select(\n",
        "        \"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \n",
        "        \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\n",
        "        \"displayName\", \"id\", \"tenantId\",\n",
        "        F.explode(\"attendanceIntervals\").alias(\"attendanceIntervalsExplode\")\n",
        "        ).select(\"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\n",
        "            \"attendanceIntervalsExplode.*\",\n",
        "            \"displayName\", \"id\", \"tenantId\")\n",
        "    # clean up column names and timestamp types\n",
        "    df_flat = df_flat.withColumnRenamed(\"id\", \"userId\").withColumnRenamed(\"displayName\", \"userDisplayName\").withColumnRenamed(\"emailAddress\", \"userEmailAddress\") \\\n",
        "        .withColumnRenamed(\"totalAttendanceInSeconds\", \"totalAttendanceInSec\").withColumnRenamed(\"tenantId\", \"userTenantId\") \\\n",
        "        .withColumnRenamed(\"joinDateTime\", \"attendanceInterval_joinDateTime\").withColumnRenamed(\"leaveDateTime\", \"attendanceInterval_leaveDateTime\").withColumnRenamed(\"durationInSeconds\", \"attendanceInterval_durationInSec\")\n",
        "    df_flat = df_flat.withColumn('meetingStartDateTime', F.to_timestamp(F.col('meetingStartDateTime'))) \\\n",
        "                .withColumn('meetingEndDateTime', F.to_timestamp(F.col('meetingEndDateTime'))) \\\n",
        "                .withColumn('attendanceInterval_joinDateTime', F.to_timestamp(F.col('attendanceInterval_joinDateTime'))) \\\n",
        "                .withColumn('attendanceInterval_leaveDateTime', F.to_timestamp(F.col('attendanceInterval_leaveDateTime')))\n",
        "    # temp: add unique primary key per row, by combining meeting IDs with user IDs\n",
        "    # this assumes every person attending a meeting has only one attendance interval\n",
        "    df_flat = df_flat.withColumn('meetingUserId_pk', F.concat(F.col('meetingId'),F.col('userId')))\n",
        "    df_flat = df_flat.select(\n",
        "        'meetingUserId_pk','meetingId','meetingStartDateTime','meetingEndDateTime','totalParticipantCount','userId','userDisplayName','userEmailAddress',\n",
        "        'userTenantId','role','totalAttendanceInSec','attendanceInterval_joinDateTime','attendanceInterval_leaveDateTime','attendanceInterval_durationInSec'\n",
        "        )\n",
        "    return df_flat\n",
        "\n",
        "def correct_graph_dataset(tables_source, write_destination):\n",
        "    items = oea.get_folders(tables_source)\n",
        "    for item in items: \n",
        "        table_path = tables_source +'/'+ item\n",
        "        if item == 'metadata.csv':\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\n",
        "        elif item == 'users':\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "            streaming_df_users = spark.readStream.format('delta').load(oea.to_url(table_path))\n",
        "            df_corrected = _correct_users_table(streaming_df_users)\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\n",
        "            query = query.start(oea.to_url(write_destination + '/' +item))\n",
        "            query.awaitTermination() \n",
        "            logger.info('Successfully corrected the users table from: ' + table_path)\n",
        "        elif item == 'm365_app_user_detail':\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "            streaming_df_m365 = spark.readStream.format('delta').load(oea.to_url(table_path))\n",
        "            df_corrected = _correct_m365_table(streaming_df_m365)\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\n",
        "            query = query.start(oea.to_url(write_destination + '/' + item))\n",
        "            query.awaitTermination() \n",
        "            logger.info('Successfully corrected the m365_app_user_detail table from: ' + table_path)\n",
        "        elif item == 'teams_activity_user_detail':\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "            streaming_df_teams = spark.readStream.format('delta').load(oea.to_url(table_path))\n",
        "            df_corrected = _correct_teams_table(streaming_df_teams)\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\n",
        "            query = query.start(oea.to_url(write_destination + '/' + item))\n",
        "            query.awaitTermination() \n",
        "            logger.info('Successfully corrected the teams_activity_user_detail table from: ' + table_path)\n",
        "        elif item == 'meeting_attendance_report':\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
        "            streaming_df_meetings = spark.readStream.format('delta').load(oea.to_url(table_path))\n",
        "            df_corrected = _correct_meetings_table(streaming_df_meetings)\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\n",
        "            query = query.start(oea.to_url(write_destination + '/' + item))\n",
        "            query.awaitTermination() \n",
        "            logger.info('Successfully corrected the meeting_attendance_report table from: ' + table_path)\n",
        "        else:\n",
        "            logger.info('No defined function for table: ' + item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if testdataSet == 'k12':\n",
        "    correct_graph_dataset('stage2/Ingested/graph_api/beta', 'stage2/Ingested_Corrected/graph_api/beta')\n",
        "    logger.info('Finished schema correction for Graph dataset')\n",
        "elif testdataSet == 'hed':\n",
        "    correct_graph_dataset('stage2/Ingested/graph_api/beta', 'stage2/Ingested_Corrected/graph_api/beta')\n",
        "    correct_graph_dataset('stage2/Ingested/graph_api/v1.0', 'stage2/Ingested_Corrected/graph_api/v1.0')\n",
        "    logger.info('Finished schema correction for Graph dataset')\n",
        "else:\n",
        "    logger.info('Unrecognized testdataSet - please choose either k12 or hed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = spark.read.format('delta').load(oea.to_url('stage2/Ingested_Corrected/graph_api/v1.0/meeting_attendance_report'), header='true')\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    }
  ],
  "metadata": {
    "description": "Used for \"2_ingest_graph\" pipeline. Corrects the schema from Graph data originally landed - flattening the nested JSON arrays, and correcting column names/data types.",
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
