{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Test for processing Microsoft Education Insights: Reading Progress data\r\n",
        "\r\n",
        "This notebook demonstrates possible data processing and exploration of the Microsoft Education Insights data, using the OEA_py class notebook. Specifically, this notebook and other Reading_Progress module assets are responsible for data manipulation pertaining to Insights reading progress. \r\n",
        "\r\n",
        "Most of the data processing done in this notebook are also achieved by executing the Reading Progress module main pipeline. This notebook is designed as an alternate approach to the same processing, as well as module data exploration and visualization. \r\n",
        "\r\n",
        "Directory landing is outlined in each step\r\n",
        "\r\n",
        "The steps are clearly outlined below:\r\n",
        "1. Set the workspace,\r\n",
        "2. Land Insights/Reading Progress Module K-12 Test Data,\r\n",
        "3. Ingest the Insights/Reading Progress Module Test Data,\r\n",
        "4. Reading Progress Schema Correction,\r\n",
        "5. Refine the Reading Progress Module Test Data, \r\n",
        "6. Demonstrate Lake Database Queries/Final Remarks, and\r\n",
        "7. Appendix"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace('sam')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.) Land Insights/Reading Progress Module K-12 Test Data\r\n",
        "\r\n",
        "Directory: ```GitHub.com (raw data) -> stage1/Transactional/M365```\r\n",
        "\r\n",
        "Run the code block for landing Insights K-12 test data (**NOTE**: This is the same test data used for this Reading Progress module).\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1) Land batch data files into stage1 of the data lake.\r\n",
        "# In this example we pull Insights/Reading Progress K-12 test csv data files from github and land it in oea/sandboxes/sam/stage1/Transactional/M365/v1.14\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/activity/2022-01-28/ApplicationUsage.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/activity', 'activity_k12_test_data.csv', oea.ADDITIVE_BATCH_DATA)\r\n",
        "\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/AadGroup/aadgroup.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/AadGroup', 'aadgroup_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/AadGroupMembership/aadgroupmembership.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/AadGroupMembership', 'aadgroupmembership_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/AadUser/aaduser.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/AadUser', 'aaduser_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/AadUserPersonMapping/aaduserpersonmapping.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/AadUserPersonMapping', 'aaduserpersonmapping_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/Course/course.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/Course', 'course_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/CourseGradeLevel/coursegradelevel.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/CourseGradeLevel', 'coursegradelevel_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/CourseSubject/coursesubject.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/CourseSubject', 'coursesubject_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/Enrollment/enrollment.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/Enrollment', 'enrollment_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/Organization/organization.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/Organization', 'organization_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/Person/person.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/Person', 'person_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonDemographic/persondemographic.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonDemographic', 'persondemographic_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonDemographicEthnicity/persondemographicethnicity.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonDemographicEthnicity', 'persondemographicethnicity_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonDemographicPersonFlag/persondemographicpersonflag.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonDemographicPersonFlag', 'persondemographicpersonflag_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonDemographicRace/persondemographicrace.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonDemographicRace', 'persondemographicrace_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonEmailAddress/personemailaddress.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonEmailAddress', 'personemailaddress_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonIdentifier/personidentifier.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonIdentifier', 'personidentifier_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonOrganizationRole/personorganizationrole.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonOrganizationRole', 'personorganizationrole_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonPhoneNumber/personphonenumber.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonPhoneNumber', 'personphonenumber_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/PersonRelationship/personrelationship.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/PersonRelationship', 'personrelationship_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/RefDefinition/refdefinition.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/RefDefinition', 'refdefinition_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/RefTranslation/reftranslation.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/RefTranslation', 'reftranslation_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/Section/section.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/Section', 'section_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/SectionGradeLevel/sectiongradelevel.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/SectionGradeLevel', 'sectiongradelevel_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/SectionSession/sectionsession.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/SectionSession', 'sectionsession_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/SectionSubject/sectionsubject.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/SectionSubject', 'sectionsubject_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/Session/session.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/Session', 'session_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)\r\n",
        "data = requests.get('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/k12_test_data/roster/2022-01-28T06-16-22/SourceSystem/sourcesystem.csv').text\r\n",
        "oea.land(data, 'M365/v1.14/SourceSystem', 'sourcesystem_k12_test_data.csv', oea.SNAPSHOT_BATCH_DATA)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.) Ingest the Insights/Reading Progress Module Test Data\r\n",
        "\r\n",
        "Directory: ```stage1/Transactional/M365 -> stage2/Ingested/reading_progress```.\r\n",
        "\r\n",
        "Both test datasets are formatted exactly as the Insights data - thus, there will be no column names or correct dtypes, initially. Ingest the data using the ```ingest_reading_prog()``` function, and next step will correct the table schemas.\r\n",
        "\r\n",
        "Purpose of this separate function is to extract Insights data originally landed in ```stage1/Transactional/M365``` and write to ```stage2/Ingested/reading_progress```, rather than ingesting into stage2/Ingested/M365 (as the ```oea.ingest()``` function would write to the same data source folder name as it was read from).\r\n",
        "\r\n",
        "**NOTE:**\r\n",
        " - AADGroupMembership table is not ingested, since it is not used for this Reading Progress module. If desired, refer to the Insights module and add the same processing as needed."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this method is almost identical to the ingest function in the OEA framework, except with the additional function to change the ingested directory \r\n",
        "def ingest_reading_prog(entity_path, write_entity_path, primary_key='id', options={}):\r\n",
        "    \"\"\" Ingests the data for the entity in the given path.\r\n",
        "        CSV files are expected to have a header row by default, and JSON files are expected to have complete JSON docs on each row in the file.\r\n",
        "        To specify options that are different from these defaults, use the options param.\r\n",
        "        eg, ingest('contoso_sis/v0.1/students') # ingests all entities found in that path\r\n",
        "        eg, ingest('contoso_sis/v0.1/students', options={'header':False}) # for CSV files that don't have a header\r\n",
        "    \"\"\"\r\n",
        "    primary_key = oea.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
        "    ingested_path = f'stage2/Ingested/{write_entity_path}'\r\n",
        "    raw_path = f'stage1/Transactional/{entity_path}'\r\n",
        "    batch_type, source_data_format = oea.get_batch_info(raw_path)\r\n",
        "    logger.info(f'Ingesting from: {raw_path}, batch type of: {batch_type}, source data format of: {source_data_format}')\r\n",
        "    source_url = oea.to_url(f'{raw_path}/{batch_type}_batch_data')\r\n",
        "\r\n",
        "    if batch_type == 'snapshot': source_url = f'{source_url}/{oea.get_latest_folder(source_url)}' \r\n",
        "            \r\n",
        "    logger.debug(f'Processing {batch_type} data from: {source_url} and writing out to: {ingested_path}')\r\n",
        "    if batch_type == 'snapshot':\r\n",
        "        def batch_func(df): oea.overwrite(df, ingested_path, primary_key)\r\n",
        "    elif batch_type == 'additive':\r\n",
        "        def batch_func(df): oea.append(df, ingested_path, primary_key)\r\n",
        "    elif batch_type == 'delta':\r\n",
        "        def batch_func(df): oea.upsert(df, ingested_path, primary_key)\r\n",
        "    else:\r\n",
        "        raise ValueError(\"No valid batch folder was found at that path (expected to find a single folder with one of the following names: snapshot_batch_data, additive_batch_data, or delta_batch_data). Are you sure you have the right path?\")                      \r\n",
        "\r\n",
        "    if options == None: options = {}\r\n",
        "    options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
        "    if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
        "\r\n",
        "    number_of_new_inbound_rows = oea.process(source_url, batch_func, options)\r\n",
        "    if number_of_new_inbound_rows > 0:    \r\n",
        "        oea.add_to_lake_db(ingested_path)\r\n",
        "    return number_of_new_inbound_rows"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) The next step is to ingest the batch data into stage2\r\n",
        "# Note that when you run this the first time, you'll see an info message like \"Number of new inbound rows processed: 2\".\r\n",
        "# If you run this a second time, the number of inbound rows processed will be 0 because the ingestion uses spark structured streaming to keep track of what data has already been processed.\r\n",
        "options = {'header':False}\r\n",
        "ingest_reading_prog(f'M365/v1.14/activity', f'reading_progress/v0.1/activity', '_c3', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/AadGroup', f'reading_progress/v0.1/AadGroup', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/AadUser', f'reading_progress/v0.1/AadUser', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/AadUserPersonMapping', f'reading_progress/v0.1/AadUserPersonMapping', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Course', f'reading_progress/v0.1/Course', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/CourseGradeLevel', f'reading_progress/v0.1/CourseGradeLevel', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/CourseSubject', f'reading_progress/v0.1/CourseSubject', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Enrollment', f'reading_progress/v0.1/Enrollment', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Organization', f'reading_progress/v0.1/Organization', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Person', f'reading_progress/v0.1/Person', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographic', f'reading_progress/v0.1/PersonDemographic', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographicEthnicity', f'reading_progress/v0.1/PersonDemographicEthnicity', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographicPersonFlag', f'reading_progress/v0.1/PersonDemographicPersonFlag', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonDemographicRace', f'reading_progress/v0.1/PersonDemographicRace', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonEmailAddress', f'reading_progress/v0.1/PersonEmailAddress', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonIdentifier', f'reading_progress/v0.1/PersonIdentifier', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonOrganizationRole', f'reading_progress/v0.1/PersonOrganizationRole', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/PersonPhoneNumber', f'reading_progress/v0.1/PersonPhoneNumber', '_c0', options)\r\n",
        "#ingest_reading_prog(f'M365/v1.14/PersonRelationship', f'reading_progress/v0.1/PersonRelationship', '_c0', options) # <- no test data currently\r\n",
        "ingest_reading_prog(f'M365/v1.14/RefDefinition', f'reading_progress/v0.1/RefDefinition', '_c0', options)\r\n",
        "#ingest_reading_prog(f'M365/v1.14/RefTranslation', f'reading_progress/v0.1/RefTranslation', '_c0', options) # <- no test data currently\r\n",
        "ingest_reading_prog(f'M365/v1.14/Section', f'reading_progress/v0.1/Section', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SectionGradeLevel', f'reading_progress/v0.1/SectionGradeLevel', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SectionSession', f'reading_progress/v0.1/SectionSession', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SectionSubject', f'reading_progress/v0.1/SectionSubject', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/Session', f'reading_progress/v0.1/Session', '_c0', options)\r\n",
        "ingest_reading_prog(f'M365/v1.14/SourceSystem', f'reading_progress/v0.1/SourceSystem', '_c0', options)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5) Now you can run queries against the auto-generated \"lake database\" with the ingested Insights/Reading Progress data.\r\n",
        "df = spark.sql(\"select * from ldb_sam_s2i_reading_progress_v0p1.activity\")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.) Reading Progress Schema Correction\r\n",
        "\r\n",
        "Directory: ```stage2/Ingested/reading_progress -> stage2/Ingested_Corrected/reading_progress```\r\n",
        "\r\n",
        "This step uses the same four functions from the \"ReadingProgress_schema_correction\" notebook, where the metadata.csv is used to correct each table's schema. Each table's schema is updated with the corrected column names and dtypes.\r\n",
        "\r\n",
        "After the schema is corrected, each table is written to stage2/Ingested_Corrected."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) schema correction, since Insights test data initially landed doesn't have column headers or correct dtypes.\r\n",
        "\r\n",
        "def _extract_element(lst, element_num=0):\r\n",
        "    return [item[element_num] for item in lst]\r\n",
        "\r\n",
        "def _dtype_config(dtype_lst):\r\n",
        "    return [item.capitalize() + 'Type()' for item in dtype_lst]\r\n",
        "\r\n",
        "def correct_insights_table_schema(df, table_name):\r\n",
        "    list_of_column_names = _extract_element(metadata[table_name])\r\n",
        "    list_of_column_dtypes = _extract_element(metadata[table_name], 1)\r\n",
        "    list_of_column_dtypes = _dtype_config(list_of_column_dtypes)\r\n",
        "\r\n",
        "    n = 0\r\n",
        "    df_updatedColumns = df\r\n",
        "    for c in df.columns:\r\n",
        "        if c != 'rundate':\r\n",
        "            new_col_name = list_of_column_names[n]\r\n",
        "            df_updatedColumns = df_updatedColumns.withColumnRenamed(c, new_col_name)\r\n",
        "            if list_of_column_dtypes[n] != 'StringType()':\r\n",
        "                if list_of_column_dtypes[n] == 'IntegerType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(IntegerType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'TimestampType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(TimestampType()))\r\n",
        "                elif list_of_column_dtypes == 'ShortType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(ShortType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'LongType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(LongType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'DoubleType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(DoubleType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'DateType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(DateType()))\r\n",
        "                elif list_of_column_dtypes[n] == 'BooleanType()':\r\n",
        "                    df_updatedColumns = df_updatedColumns.withColumn(new_col_name, df_updatedColumns[new_col_name].cast(BooleanType()))\r\n",
        "        else:\r\n",
        "            df_updatedColumns = df_updatedColumns\r\n",
        "        n = n + 1\r\n",
        "    return df_updatedColumns\r\n",
        "\r\n",
        "def correct_reading_progress_dataset(tables_source, write_destination):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        if item == 'metadata.csv':\r\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
        "        else:\r\n",
        "            table_path = tables_source +'/'+ item\r\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "            streaming_df = spark.readStream.format('delta').load(oea.to_url(table_path))\r\n",
        "            df_corrected = correct_insights_table_schema(streaming_df, table_name=item)\r\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\r\n",
        "            query = query.start(oea.to_url(write_destination + '/' +item))\r\n",
        "            query.awaitTermination() \r\n",
        "            logger.info('Successfully corrected the schema for table: ' + item + ' from: ' + table_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Microsoft_Education_Insights/test_data/metadata.csv')\r\n",
        "correct_reading_progress_dataset('stage2/Ingested/reading_progress/v0.1', 'stage2/Ingested_Corrected/reading_progress/v0.1')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('delta').load(oea.to_url('stage2/Ingested_Corrected/reading_progress/v0.1/activity'), header='true')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.) Refine the Reading Progress Module Test Data\r\n",
        "\r\n",
        "Directory: ```stage2/Ingested_Corrected/reading_progress -> stage2/Refined/reading_progress```\r\n",
        "\r\n",
        "This step then refines the Insights test data from stage2/Ingested_Corrected to stage2/Refined, using the metadata.csv. This step is responsible for :\r\n",
        " - pseudonymization (which preserves sensitive student information by either hashing or masking the sensitive columns), and \r\n",
        " - data transformation (to fit the Reading Progress module schema). \r\n",
        "\r\n",
        "Tables are separated into either ```stage2/Refined/reading_progress/v0.1/general``` or ```stage2/Refined/reading_progress/v0.1/sensitive```, depending on whether each table is pseudonymized or has a sensitive column-hashing/masking mapping, respectively.\r\n",
        "\r\n",
        "\r\n",
        "**To-Do's:**\r\n",
        " - Find workaround for creating lookup tables, when the primary key is un-hashed after pseudonymization \r\n",
        "    * (i.e. *affected tables*: PersonDemographicEthnicity, PersonDemographicPersonFlag, PersonDemographicRace, PersonEmailAddress, PersonIdentifier, PersonOrganizationRole, and PersonPhoneNumber). \r\n",
        " - Resolve ingesting and refining AadGroupMembership table."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) this step refines the data through the use of metadata (this is where the pseudonymization of the data occurs).\r\n",
        "def refine_reading_prog_corrected(df, table_name, metadata=None, primary_key='id'):\r\n",
        "    source_path = f'stage2/Ingested_Corrected/reading_progress/v0.1/activity'\r\n",
        "    sink_general_path = f'stage2/Refined/reading_progress/v0.1/general/{table_name}'\r\n",
        "    sink_sensitive_path = f'stage2/Refined/reading_progress/v0.1/sensitive/{table_name}_lookup'\r\n",
        "\r\n",
        "    # NOTE: Currently does not accomodate change data; this is expected to be updated for production purposes\r\n",
        "    #df_changes = oea.get_latest_changes(source_path, sink_general_path)\r\n",
        "    spark_schema = oea.to_spark_schema(metadata)\r\n",
        "    df = oea.modify_schema(df, spark_schema)        \r\n",
        "\r\n",
        "    if df.count() > 0:\r\n",
        "        df_pseudo, df_lookup = oea.pseudonymize(df, metadata)\r\n",
        "        oea.upsert(df_pseudo, sink_general_path, primary_key)\r\n",
        "        oea.upsert(df_lookup, sink_sensitive_path, primary_key)\r\n",
        "        oea.add_to_lake_db(sink_general_path)\r\n",
        "        oea.add_to_lake_db(sink_sensitive_path)\r\n",
        "        logger.info(f'Processed {df.count()} rows from {source_path} into stage2/Refined')\r\n",
        "    else:\r\n",
        "        logger.info(f'No updated rows in {source_path} to process.')\r\n",
        "    return df.count()\r\n",
        "\r\n",
        "def refine_reading_progress_dataset(tables_source):\r\n",
        "    # read in relevant tables for data transformation\r\n",
        "    base_path = tables_source\r\n",
        "    df_activity = oea.load(base_path + '/activity')\r\n",
        "    df_aaduserpersonmapping = oea.load(base_path + '/AadUserPersonMapping')\r\n",
        "    df_person = oea.load(base_path + '/Person/')\r\n",
        "    df_personOrgRole = oea.load(base_path + '/PersonOrganizationRole')\r\n",
        "    df_organization = oea.load(base_path + '/Organization')\r\n",
        "    df_refDefinition = oea.load(base_path + '/RefDefinition')\r\n",
        "    # separate student frame, subset and refine data\r\n",
        "    dfStudent = df_personOrgRole.join(df_person, df_personOrgRole.PersonId == df_person.Id, how='inner')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'RefRoleId', 'RefGradeLevelId', 'OrganizationId')\r\n",
        "    dfStudent = dfStudent.join(df_organization, dfStudent.OrganizationId == df_organization.Id, how='inner').withColumnRenamed('Name', 'OrganizationName')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'RefRoleId', 'RefGradeLevelId', 'OrganizationId', 'OrganizationName')\r\n",
        "    dfStudent = dfStudent.join(df_refDefinition, dfStudent.RefRoleId == df_refDefinition.Id, how='inner').withColumnRenamed('Code', 'PersonRole')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'RefGradeLevelId', 'OrganizationId', 'OrganizationName')\r\n",
        "    dfStudent = dfStudent.filter(dfStudent['PersonRole'] == 'Student')\r\n",
        "    dfStudent = dfStudent.join(df_refDefinition, dfStudent.RefGradeLevelId == df_refDefinition.Id, how='left').withColumnRenamed('Code', 'StudentGrade')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'StudentGrade', 'OrganizationId', 'OrganizationName')\r\n",
        "    df_aaduserpersonmapping = df_aaduserpersonmapping.withColumnRenamed('PersonId', 'id')\r\n",
        "    dfStudent = dfStudent.join(df_aaduserpersonmapping, dfStudent.PersonId == df_aaduserpersonmapping.id, how='inner').withColumnRenamed('ObjectId', 'AadUserId')\r\n",
        "    dfStudent = dfStudent.select('PersonId', 'AadUserId', 'Surname', 'GivenName', 'MiddleName', 'PersonRole', 'StudentGrade', 'OrganizationId', 'OrganizationName')\r\n",
        "\r\n",
        "    refine_reading_prog_corrected(dfStudent, 'Student', metadata['Student'], 'PersonId_pseudonym')\r\n",
        "    # refine reading progress data from Insights activity table\r\n",
        "    dfReadingProgress = df_activity.where(\"AppName == 'ReadingProgress'\")\r\n",
        "    dfReadingProgress = dfReadingProgress.select('ActorId', 'SignalId', 'SignalType', 'StartTime', 'AppName', 'Action', 'ClassId', 'ReadingSubmissionWordsPerMinute', 'ReadingSubmissionAccuracyScore', \\\r\n",
        "                                        'ReadingSubmissionRepetitionsCount', 'ReadingSubmissionInsertionsCount', 'ReadingSubmissionMispronunciationCount', 'ReadingSubmissionObmissionCount', 'ReadingSubmissionAttemptNumber', \\\r\n",
        "                                        'ReadingAssignmentWordCount', 'ReadingAssignmentFleschKincaidGradeLevel', 'ReadingAssignmentLanguage')\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumnRenamed('ActorId', 'AadUserId').withColumnRenamed('ClassId', 'AadGroupId')\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionAccuracyScore', dfReadingProgress['ReadingSubmissionAccuracyScore'].cast(DoubleType()))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionRepetitionsRate', F.col('ReadingSubmissionRepetitionsCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionRepetitionsRate', F.round(F.col('ReadingSubmissionRepetitionsRate'), 3))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionMispronunciationRate', F.col('ReadingSubmissionMispronunciationCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionMispronunciationRate', F.round(F.col('ReadingSubmissionMispronunciationRate'), 3))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionInsertionsRate', F.col('ReadingSubmissionInsertionsCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionInsertionsRate', F.round(F.col('ReadingSubmissionInsertionsRate'), 3))\r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionObmissionRate', F.col('ReadingSubmissionObmissionCount')/F.col('ReadingAssignmentWordCount') * 100) \r\n",
        "    dfReadingProgress = dfReadingProgress.withColumn('ReadingSubmissionObmissionRate', F.round(F.col('ReadingSubmissionObmissionRate'), 3))\r\n",
        "\r\n",
        "    try:\r\n",
        "        refine_reading_prog_corrected(dfReadingProgress, 'ReadingProgress_activity', metadata['ReadingProgress_activity'], 'SignalId')\r\n",
        "    except AnalysisException as e:\r\n",
        "        # This means the table may have not been properly refined due to errors with the primary key not aligning with columns expected in the lookup table.\r\n",
        "        pass\r\n",
        "    \r\n",
        "    logger.info('Finished refining Reading Progress tables.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = oea.get_metadata_from_url('https://raw.githubusercontent.com/microsoft/OpenEduAnalytics/main/modules/module_catalog/Reading_Progress/data/metadata.csv')\r\n",
        "refine_reading_progress_dataset('stage2/Ingested_Corrected/reading_progress/v0.1')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oea.add_to_lake_db('stage2/Refined/reading_progress/v0.1/general/ReadingProgress_activity')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('delta').load(oea.to_url('stage2/Refined/reading_progress/v0.1/general/Student'), header='true')\r\n",
        "display(df.limit(10))\r\n",
        "df = spark.read.format('delta').load(oea.to_url('stage2/Refined/reading_progress/v0.1/general/ReadingProgress_activity'), header='true')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.) Demonstrate Lake Database Queries/Final Remarks"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to reset this example (deleting all the example Insights data in your workspace)\r\n",
        "oea.rm_if_exists('stage1/Transactional/M365')\r\n",
        "oea.rm_if_exists('stage2/Ingested/reading_progress')\r\n",
        "oea.rm_if_exists('stage2/Ingested_Corrected/reading_progress')\r\n",
        "oea.rm_if_exists('stage2/Refined/reading_progress')\r\n",
        "oea.drop_lake_db('ldb_sam_s2i_reading_progress_v0p1')\r\n",
        "oea.drop_lake_db('ldb_sam_s2r_reading_progress_v0p1')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate an initial metadata file for manual modification\r\n",
        "metadata = oea.create_metadata_from_lake_db('ldb_sam_s2i_reading_progress_v0p1')\r\n",
        "dlw = DataLakeWriter(oea.to_url('stage1/Transactional/reading_progress'))\r\n",
        "dlw.write('metadata.csv', metadata)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sql db for the ingested Reading Progress data\r\n",
        "oea.create_sql_db('stage2/Ingested/reading_progress')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}